
## Future Ideas

### Dataset (Step ? far future)

1. Salsa dump
2. Debian mailing list dump

### Training (Step ? far future)

Pick an open-access LLM to fine-tune with LoRA. The concrete choise of a baseline LLM is to be investigated (e.g., should we start from pre-trained LLM or fine-tuned chatting LLM?).
The additional instruct tuning and RLHF steps are to be investigated.

Possible solutions include LoRA and RAG.

2. LoRA paper
3. InstructGPT paper

## References

1. https://lists.debian.org/debian-project/2023/12/msg00028.html

