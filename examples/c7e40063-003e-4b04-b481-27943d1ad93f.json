[
  {
    "role": "user",
    "content": "The following is a file named debgpt/llm.py:\n```\n# Copyright (C) 2024 Mo Zhou <lumin@debian.org>\n# MIT/Expat License.\nimport json\nimport os\nimport re\nfrom prompt_toolkit import prompt\nfrom transformers import pipeline, Conversation\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport numpy as np\nimport torch as th\nfrom typing import *\nimport argparse\nimport rich\nfrom rich.panel import Panel\nconsole = rich.get_console()\n\n\nclass AbstractLLM(object):\n    def __init__(self):\n        self.device = 'cuda' if th.cuda.is_available() else 'cpu'\n\n    @th.no_grad()\n    def generate(self, messages: Union[list, str]):\n        # Used by backend.py for serving a client\n        raise NotImplementedError\n\n    @th.no_grad()\n    def __call__(self, *args, **kwargs):\n        return self.generate(*args, **kwargs)\n\n    def chat(self):\n        # chat with LLM locally\n        raise NotImplementedError\n\n\nclass Mistral7B(AbstractLLM):\n    '''\n    https://docs.mistral.ai/models/\n    https://huggingface.co/docs/transformers/model_doc/mistral\n    https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n    https://huggingface.co/docs/transformers/main/chat_templating\n\n    TODO: also support 4bit and 8bit for CPU inference. Not everybody has expensive GPUs.\n    '''\n    model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n\n    def __init__(self, *, torch_dtype=th.float16):\n        '''\n        torch_dtype: th.float32 requires 32GB CUDA memory.\n                     th.float16/th.bfloat16 requires 16GB CUDA memory.\n                     th.float16 has better hardware compatibility than bfloat16.\n                     th.float16 has better compatibility than bfloat16.\n        '''\n        super().__init__()\n        console.log(f'Mistral7B> Loading {self.model_id} (float16)')\n        self.llm = AutoModelForCausalLM.from_pretrained(self.model_id,\n                                                        torch_dtype=torch_dtype)\n        self.llm.to(self.device)\n        self.tok = AutoTokenizer.from_pretrained(self.model_id)\n        self.kwargs = {'max_new_tokens': 512,\n                       'do_sample': True,\n                       'pad_token_id': 2}\n\n    @th.no_grad()\n    def generate(self, messages: List[Dict]):\n        encoded = self.tok.apply_chat_template(messages, tokenize=True,\n                                               return_tensors='pt',\n                                               add_generation_prompt=True).to(0)\n        model_inputs = encoded.to(self.device)\n        input_length = model_inputs.shape[1]\n        generated_ids = self.llm.generate(model_inputs, **self.kwargs)\n        generated_new_ids = generated_ids[:, input_length:]\n        generated_new_text = self.tok.batch_decode(generated_new_ids,\n                                                   skip_special_tokens=True,\n                                                   clean_up_tokenization_spaces=True)[0]\n        new_message = {'role': 'assistant', 'content': generated_new_text}\n        messages.append(new_message)\n        return messages\n\n    def chat(self, chat=Conversation()):\n        '''\n        https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.ConversationalPipeline\n        '''\n        pipe = pipeline('conversational', model=self.llm,\n                        tokenizer=self.tok, device=self.device)\n        try:\n            while text := prompt('Prompt> '):\n                chat.add_message({'role': 'user', 'content': text})\n                chat = pipe(chat, **self.kwargs)\n                console.print('LLM> ', chat[-1]['content'])\n        except EOFError:\n            pass\n        except KeyboardInterrupt:\n            pass\n        return chat\n\n\ndef create_llm(args) -> AbstractLLM:\n    # factory\n    if args.llm == 'Mistral7B':\n        model = Mistral7B()\n        model.kwargs['max_new_tokens'] = args.max_new_tokens\n    else:\n        raise NotImplementedError(f'{args.llm} is not yet implemented')\n    return model\n\n\nif __name__ == '__main__':\n    ag = argparse.ArgumentParser('Chat with LLM locally.')\n    ag.add_argument('--max_new_tokens', type=int, default=512,\n                    help='max length of new token sequences added by llm')\n    ag.add_argument('--debgpt_home', type=str,\n                    default=os.path.expanduser('~/.debgpt'))\n    ag.add_argument('--llm', type=str, default='Mistral7B',\n                    choices=('Mistral7B',))\n    ag.add_argument('-i', '--ipython', action='store_true')\n    ag = ag.parse_args()\n    console.log(ag)\n\n    if not os.path.exists(ag.debgpt_home):\n        os.mkdir(ag.debgpt_home)\n\n    # for debugging\n    if ag.ipython:\n        llm = create_llm(ag)\n        # XXX: ipython here is for debugging\n        msg = [{'role': 'user', 'content': 'hi!'}]\n        import IPython\n        IPython.embed(colors='neutral')\n        exit()\n\n    # load model and start chat\n    llm = create_llm(ag)\n    log = llm.chat()\n\n    # save a record\n    fpath = os.path.join(ag.debgpt_home, str(log.uuid) + '.json')\n    with open(fpath, 'wt') as f:\n        json.dump(log.messages, f)\n\n    console.log(f'LLM chat session saved at {fpath}')\n```\nWhat is the copyright and license of this file? Use SPDX format."
  },
  {
    "role": "assistant",
    "content": "Copyright (C) 2024 Mo Zhou <lumin@debian.org>\n\nMIT License\n----------\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE."
  },
  {
    "role": "user",
    "content": "I just need the SPDX identifier. Nothing else, please"
  },
  {
    "role": "assistant",
    "content": "MIT"
  },
  {
    "role": "user",
    "content": "that looks good. By the way, do you know the difference between GPL-2.0 and GPL-3.0? please explain briefly"
  },
  {
    "role": "assistant",
    "content": "Yes, I can explain the differences between the GNU General Public License version 2 (GPLv2) and version 3 (GPLv3) briefly. Both GPLv2 and GPLv3 are popular open-source licenses, but they have some key differences:\n\n1. **Linking Exception**: The GPLv2 requires that every part of a combined work be released under the same license. This means that if you use a library under GPLv2, the entire application that uses the library must be released as well. However, GPLv3 has a stronger linking exception which allows you to statically link a GPLv3-licensed library against another library without making the entire application GPLv3.\n2. **Additional Requirements**: GPLv3 has additional requirements compared to GPLv2. Some of these requirements include the requirement for distributors to provide the source code of a modified work or a copy of the original licenses, as well as offering recipients of the software the choice of getting a machine-readable copy of the source code in a suitable format.\n3. **Protection against trolling**: GPLv3 has a more robust protection against trolling and patent aggression. Section 11 of GPLv3 requires distributors to provide a license granting access to the patent claims they control, which are essential to run the licensed software. This aims to prevent patent holders from taking legal action against users and distributors under the threat of patent infringement.\n4. **Compatibility**: The LGPL (Library GPL) and MPL (Mozilla Public License) are less restrictive than GPL and more compatible with proprietary solutions. Because of these compatibility issues, GPLv3 might not be the best choice for projects that are intended to be integrated with proprietary or closed-source software.\n\nIt's important to note that the specific circumstances and goals of your project will determine which license is the best fit. Carefully evaluating your options and consulting a legal expert if needed is recommended."
  }
]