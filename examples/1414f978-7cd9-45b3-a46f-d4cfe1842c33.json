[
  {
    "role": "system",
    "content": "You are an excellent free software developer. You write high-quality code. You aim to provide people with prefessional and accurate information. You cherrish software freedom. You obey the Debian Social Contract and the Debian Free Software Guideline. You follow the Debian Policy."
  },
  {
    "role": "user",
    "content": "The following is the content of a file named `debgpt/main_cli.py`, enclosed by the \"```\" mark:\n```\n# Copyright (C) 2024 Mo Zhou <lumin@debian.org>\n# MIT/Expat License.\n\n# suppress all warnings.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom prompt_toolkit.styles import Style\nfrom rich.markup import escape\nfrom rich.status import Status\nfrom rich.panel import Panel\nfrom prompt_toolkit import prompt, PromptSession\nimport argparse\nimport os\nimport sys\nfrom . import frontend\nfrom . import debian\nfrom . import defaults\nimport rich\nconsole = rich.get_console()\n\n\ndef task_backend(ag) -> None:\n    from . import backend\n    b = backend.create_backend(ag)\n    try:\n        b.server()\n    except KeyboardInterrupt:\n        pass\n    console.log('Server shut down.')\n    exit(1)\n\n\ndef parse_args():\n    '''\n    argparse with subparsers\n    '''\n    conf = defaults.Config()\n    ag = argparse.ArgumentParser()\n\n    # LLM inference arguments\n    ag.add_argument('--temperature', '-T', type=float, default=conf['temperature'])\n    ag.add_argument('--top_p', '-P', type=float, default=conf['top_p'])\n\n    # general frontend\n    ag.add_argument('--debgpt_home', type=str, default=conf['debgpt_home'])\n    ag.add_argument('--frontend', '-F', type=str, default=conf['frontend'], choices=('dryrun', 'zmq', 'openai'))\n\n    # ZMQ frontend\n    ag.add_argument('--backend', '-B', type=str, default=conf['backend'], help='the frontend endpoint')\n\n    # openai frontend\n    ag.add_argument('--stream', '-S', type=bool, default=conf['stream'],\n                    help='default to streaming mode when openai frontend is used')  # FIXME: this argument does not work\n    ag.add_argument('--openai_model_id', '-M', type=str, default=conf['openai_model_id'])\n\n    # CLI behavior\n    ag.add_argument('--quit', '-Q', action='store_true', help='directly quit after receiving the first response from LLM, instead of staying in interation.')\n    ag.add_argument('--multiline', action='store_true', help='enable multi-line input for prompt_toolkit. use Meta+Enter to accept the input instead.')\n    ag.add_argument('--hide_first_prompt', '-H', action='store_true', help='hide the first (generated) prompt')\n\n    # The following are task-specific subparsers\n    subps = ag.add_subparsers(help='task help')\n    ag.set_defaults(func=lambda ag: None)  # if no subparser specified\n\n    # -- backend (special mode)\n    ps_backend = subps.add_parser('backend', help='special mode: start backend server (self-hosted LLM inference)')\n    ps_backend.add_argument('--port', '-p', type=int, default=11177,\n                            help='\"11177\" looks like \"LLM\"')\n    ps_backend.add_argument('--host', type=str, default='tcp://*')\n    ps_backend.add_argument('--backend_impl', type=str,\n                            default='zmq', choices=('zmq',))\n    ps_backend.add_argument('--max_new_tokens', type=int, default=512)\n    ps_backend.add_argument('--llm', type=str, default='Mistral7B')\n    ps_backend.add_argument('--device', type=str, default='cuda')\n    ps_backend.add_argument('--precision', type=str, default='fp16')\n    ps_backend.set_defaults(func=task_backend)\n\n    # -- none (special mode)\n    # FIXME: deprecated.\n    ps_none = subps.add_parser('none', help='degenerate into general chat without debian specific stuff')\n    ps_none.set_defaults(func=lambda ag: None)\n\n    # -- stdin\n    ps_stdin = subps.add_parser('stdin', help='read stdin. special mode. no actions to be specified.')\n    ps_stdin.set_defaults(func=lambda ag: debian.stdin())\n\n    # -- mailing list\n    ps_ml = subps.add_parser('ml', help='mailing list')\n    ps_ml.add_argument('--url', '-u', type=str, required=True)\n    ps_ml.add_argument('--raw', action='store_true', help='use raw html')\n    ps_ml.add_argument('action', type=str, choices=debian.mailing_list_actions)\n    ps_ml.set_defaults(func=lambda ag: debian.mailing_list(ag.url, ag.action, raw=ag.raw))\n\n    # --bts\n    ps_bts = subps.add_parser('bts', help='BTS')\n    ps_bts.add_argument('--id', '-x', type=str, required=True)\n    ps_bts.add_argument('--raw', action='store_true', help='use raw html')\n    ps_bts.add_argument('action', type=str, default=debian.bts_actions)\n    ps_bts.set_defaults(func=lambda ag: debian.bts(ag.id, ag.action, raw=ag.raw))\n\n    # -- buildd\n    ps_buildd = subps.add_parser('buildd', help='buildd')\n    ps_buildd.add_argument('--package', '-p', type=str, required=True)\n    ps_buildd.add_argument('--suite', '-s', type=str, default='sid')\n    ps_buildd.add_argument('--raw', action='store_true', help='use raw html')\n    ps_buildd.add_argument('action', type=str, choices=debian.buildd_actions)\n    ps_buildd.set_defaults(func=lambda ag: debian.buildd(ag.package, ag.action, suite=ag.suite, raw=ag.raw))\n\n    # -- file\n    # e.g., license check (SPDX format), code improvement, code explain\n    # TODO: support multiple files (nargs=+)\n    ps_file = subps.add_parser('file', help='ask questions regarding a specific file')\n    ps_file.add_argument('--file', '-f', type=str, required=True)\n    ps_file.add_argument('action', type=str, choices=debian.file_actions)\n    ps_file.set_defaults(func=lambda ag: debian.file(ag.file, ag.action))\n\n    # -- vote\n    ps_vote = subps.add_parser('vote', help='vote.debian.org')\n    ps_vote.add_argument('--suffix', '-s', type=str, required=True,\n                    help='for example, 2023/vote_002')\n    ps_vote.add_argument('action', type=str, choices=debian.vote_actions)\n    ps_vote.set_defaults(func=lambda ag: debian.vote(ag.suffix, ag.action))\n\n    # -- policy\n    ps_policy = subps.add_parser('policy', help='policy document (plain text)')\n    ps_policy.add_argument('--section', '-s', type=str, required=True)\n    ps_policy.add_argument('action', type=str, choices=debian.policy_actions)\n    ps_policy.set_defaults(func=lambda ag: debian.policy(ag.section, ag.action))\n\n    # -- devref\n    ps_devref = subps.add_parser('devref', help='devref document')\n    ps_devref.add_argument('--section', '-s', type=str, required=True)\n    ps_devref.add_argument('action', type=str, choices=debian.devref_actions)\n    ps_devref.set_defaults(func=lambda ag: debian.devref(ag.section, ag.action))\n\n    # -- man page\n    ps_man = subps.add_parser('man', help='manual page')\n    ps_man.add_argument('--man', '-m', type=str, required=True)\n    ps_man.add_argument('action', type=str, choices=debian.man_actions)\n    ps_man.set_defaults(func=lambda ag: debian.man(ag.man, ag.action))\n\n    # -- dev mode\n    ps_dev = subps.add_parser('dev', aliases=['x'], help='code editing with context')\n    ps_dev.add_argument('--file', '-f', type=str, required=True,\n                        help='path to file you want to edit')\n    ps_dev.add_argument('--policy', type=str, default=None,\n                        help='which section of policy to look at?')\n    ps_dev.add_argument('action', type=str, choices=debian.dev_actions)\n    ps_dev.set_defaults(func=lambda ag: debian.dev(ag.file, ag.action,\n                        policy=ag.policy, debgpt_home=ag.debgpt_home))\n\n    # -- parse and sanitize\n    ag = ag.parse_args()\n    if ag.frontend == 'zmq' and ag.stream == True:\n        console.log('disabling streaming because it is not yet supported for ZMQ frontend')\n        ag.stream = False\n    return ag\n\n\ndef main():\n    # parse args and prepare debgpt_home\n    ag = parse_args()\n    console.log(ag)\n    if not os.path.exists(ag.debgpt_home):\n        os.mkdir(ag.debgpt_home)\n\n    # create task-specific prompts. note, some special tasks will exit()\n    # in their subparser default function when then finished, such as backend\n    msg = ag.func(ag)\n    f = frontend.create_frontend(ag)\n\n    # in dryrun mode, we simply print the generated initial prompts\n    # then the user can copy the prompt, and paste them into web-based\n    # LLMs like the free web-based ChatGPT (OpenAI), claude.ai (Anthropic),\n    # Bard (google), Gemini (google), huggingchat (huggingface), etc.\n    if ag.frontend == 'dryrun':\n        console.print(msg)\n        exit()\n\n    # print the prompt and do the first query, if specified\n    if msg is not None:\n        if not ag.hide_first_prompt:\n            console.print(Panel(escape(msg), title='Initial Prompt'))\n\n        # query the backend\n        if ag.stream:\n            console.print(\n                f'[bold green]LLM [{1+len(f.session)}]>[/bold green] ', end='')\n            reply = f(msg)\n        else:\n            with Status('LLM', spinner='line'):\n                reply = f(msg)\n            console.print(Panel(escape(reply), title='LLM Reply'))\n        # console.print('LLM>', reply)\n\n    # drop the user into interactive mode if specified (-i)\n    if not ag.quit:\n        # create prompt_toolkit style\n        prompt_style = Style(\n            [('prompt', 'bold fg:ansibrightcyan'), ('', 'bold ansiwhite')])\n        prompt_session = PromptSession(style=prompt_style, multiline=ag.multiline)\n        try:\n            while text := prompt_session.prompt(f'{os.getlogin()} [{len(f.session)}]> '):\n                if ag.stream:\n                    console.print(\n                        f'[bold green]LLM [{1+len(f.session)}]>[/bold green] ', end='')\n                    reply = f(text)\n                else:\n                    with Status('LLM', spinner='line'):\n                        reply = f(text)\n                    console.print(Panel(escape(reply), title='LLM Reply'))\n                # console.print('LLM>', reply)\n        except EOFError:\n            pass\n        except KeyboardInterrupt:\n            pass\n\n    # dump session to json\n    f.dump()\n\n    # some notifications\n    if any(x in sys.argv for x in ('vote',)):\n        # sensitive category\n        console.print(Panel('''[bold white on red]LLM may hallucinate and generate incorrect contents. Please further judge the correctness of the information, and do not let LLM mislead your decision on sensitive tasks, such as debian voting.[/bold white on red]''', title='!!! Warning !!!'))\n    else:\n        pass\n\n\nif __name__ == '__main__':\n    main()\n```\n\nRead this file carefully.\nLater I will ask you to edit the provided file `debgpt/main_cli.py` and generate a unix-format patch. The other information, if provided, might help you make the specified edits later. Please be quiet for now and wait for the instruction."
  },
  {
    "role": "assistant",
    "content": "Sure, I have read the content of the `debgpt/main_cli.py` file. I will wait for your instructions on editing the file and generating a unix-format patch."
  },
  {
    "role": "user",
    "content": "I have a new submodule named `debgpt/replay.py`. it provides a replay(json_file_path: str) function. Please help me add a subparser to call this function through `replay` task"
  },
  {
    "role": "assistant",
    "content": "Certainly! I can help you add a subparser for the `replay` task in `debgpt/main_cli.py`. Here's the modified code with the `replay` subparser added:\n\n```python\n# ... existing code ...\n\n# -- replay\nps_replay = subps.add_parser('replay', help='replay a conversation from a JSON file')\nps_replay.add_argument('json_file_path', type=str, help='path to the JSON file')\nps_replay.set_defaults(func=lambda ag: frontend.replay(ag.json_file_path))\n\n# ... existing code ...\n\n```\n\nThis code adds a new subparser called `replay` that takes a mandatory positional argument `json_file_path`, which represents the path to the JSON file containing the conversation to replay. It sets the `func` attribute to call the `replay` function from the `frontend` submodule with the provided JSON file path.\n\nPlease let me know if you need any further modifications or if you have any other requirements."
  },
  {
    "role": "user",
    "content": "the replay functiom is imported through `from . import replay`. it is not from the frontend namespace"
  },
  {
    "role": "assistant",
    "content": "Apologies for the confusion. Here's the modified code to import and call the `replay` function from the `debgpt.replay` submodule:\n\n```python\n# ... existing code ...\n\nfrom . import replay\n\n# ... existing code ...\n\n# -- replay\nps_replay = subps.add_parser('replay', help='replay a conversation from a JSON file')\nps_replay.add_argument('json_file_path', type=str, help='path to the JSON file')\nps_replay.set_defaults(func=lambda ag: replay.replay(ag.json_file_path))\n\n# ... existing code ...\n\n```\n\nIn this code, we import the `replay` function from the `debgpt.replay` submodule using `from . import replay`. Then, we set the `func` attribute of the `replay` subparser to call the imported `replay` function with the provided JSON file path.\n\nLet me know if you have any further questions or if there's anything else I can assist you with."
  }
]